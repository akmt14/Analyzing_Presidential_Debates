{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Presidential Debates\n",
    "\n",
    "## Group Members - Purvi Thakor, Junior Ovince, Akshay Kamath\n",
    "<br>\n",
    "With the midterms around the corner & politics dominating the news, we decided to analyze Presidential debates for our NLP project. We found datasets available on [UC Barbara's The American Presidency Project](https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/presidential-candidates-debates-1960-2016).\n",
    "\n",
    "We discussed & decided to analyze the presidential debates from 2008 onwards which meant we worked on 3 sets of debates - \n",
    "\n",
    "* <font color=blue>Obama<font color=black> v <font color=red>McCain<font color=black> _(2008)_\n",
    "  -  [Presidential Debate in Hempstead, New York](https://www.presidency.ucsb.edu/ws/index.php?pid=84526)\n",
    "  -  [Presidential Debate in Nashville, Tennessee](https://www.presidency.ucsb.edu/ws/index.php?pid=84482)\n",
    "  -  [Presidential Debate in Oxford, Mississippi](https://www.presidency.ucsb.edu/ws/index.php?pid=78691)\n",
    "<br>\n",
    "<br>\n",
    "* <font color=blue>Obama<font color=black> v <font color=red>Romney<font color=black> _(2012)_\n",
    "  -  [Presidential Debate at Lynn University in Boca Raton, Florida](https://www.presidency.ucsb.edu/ws/index.php?pid=102344)\n",
    "  -  [Presidential Debate at Hofstra University in Hempstead, New York](https://www.presidency.ucsb.edu/ws/index.php?pid=102343)\n",
    "  -  [Presidential Debate at the University of Denver, Colorado](https://www.presidency.ucsb.edu/ws/index.php?pid=102317)\n",
    "<br>\n",
    "<br>\n",
    "* <font color=blue>Clinton<font color=black> v <font color=red>Trump<font color=black> _(2016)_\n",
    "  -  [Presidential Debate at the University of Nevada, Las Vegas](https://www.presidency.ucsb.edu/ws/index.php?pid=119039)\n",
    "  -  [Presidential Debate at Washington University in St. Louis, Missouri](https://www.presidency.ucsb.edu/ws/index.php?pid=119038)\n",
    "  -  [Presidential Debate at Hofstra University in Hempstead, New York](https://www.presidency.ucsb.edu/ws/index.php?pid=119012)\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "Furthermore, we decided to analyze both the victory & concession speeches for all the candidates.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The data was manually scraped from the abovementioned site & stored in text files. Thankfully, the format across each of the documents was more or less the same & we ended up saving a lot of time in the initial few steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import tabulate\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.stem import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkdir = os.chdir(\"D:/Project/NLP/presidential_debates/\")\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename).read()\n",
    "    return input_file_text\n",
    "\n",
    "file_list=[]\n",
    "for file in os.listdir(wrkdir):\n",
    "    file_list.append(file) \n",
    "\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in file_list:\n",
    "    file = open(i,'rt')\n",
    "    file_read = file.read()\n",
    "    print(\"\\n\")\n",
    "    print(125*\"+\")\n",
    "    print(file.name)\n",
    "    print(125*\"+\")\n",
    "    print(\"\\n\")\n",
    "    file.close()\n",
    "    print(file_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the format of the files is slightly different. Initially, the conversation begins with the speaker's name in proper format & then it gets converted to upper format. So we realized that each document needs to be cleansed separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 2008 Presidential Debate Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "for i in file_list:\n",
    "    if '2008' in i:\n",
    "        file = open(i,'rt')\n",
    "        file_read = file.read()\n",
    "        file_read = file_read.replace('\\n', '')\n",
    "        all_files.append(file_read)\n",
    "        print(\"\\n\")\n",
    "        print(125*\"+\")\n",
    "        print(file.name)\n",
    "        print(125*\"+\")\n",
    "        print(\"\\n\")\n",
    "        file.close()\n",
    "        print(file_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_files = ' '.join(all_files) #converting list to string\n",
    "cleaned_files = string_files.replace('\\n\\n','') #replacing new line tags\n",
    "file_read1 = cleaned_files.replace('McCain:','MCCAIN:') #replacing one instance of proper string with upper\n",
    "file_read2 = file_read1.replace('Obama:','OBAMA:') #replacing one instance of proper string with upper\n",
    "JM_2008 = re.findall(r'MCCAIN:(.*?)(?:OBAMA:|LEHRER:|SCHIEFFER:|BROKAW:)', file_read2) #extracting comments made by McCain only\n",
    "BO_2008 = re.findall(r'OBAMA:(.*?)(?:MCCAIN:|LEHRER:|SCHIEFFER:|BROKAW:)', file_read2) #extracting comments made by Obama only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JM_2008_wc = ''.join(JM_2008)\n",
    "BO_2008_wc = ''.join(BO_2008)\n",
    "print(\"Barack Obama's word count in the 2008 presidential debates is \" + str(\"{:,}\".format(len(BO_2008_wc))) + \" words.\")\n",
    "print(\"John McCain's word count in the 2008 presidential debates is \" + str(\"{:,}\".format(len(JM_2008_wc))) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 2012 Presidential Debate Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "for i in file_list:\n",
    "    if '2012' in i:\n",
    "        file = open(i,'rt')\n",
    "        file_read = file.read()\n",
    "        file_read = file_read.replace('\\n', '')\n",
    "        all_files.append(file_read)\n",
    "        print(\"\\n\")\n",
    "        print(125*\"+\")\n",
    "        print(file.name)\n",
    "        print(125*\"+\")\n",
    "        print(\"\\n\")\n",
    "        file.close()\n",
    "        print(file_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_files = ' '.join(all_files) #converting list to string\n",
    "cleaned_files = string_files.replace('\\n\\n','') #replacing new line tags\n",
    "file_read1 = cleaned_files.replace('Gov. Romney.',' ROMNEY:') #replacing all instances of proper string with upper\n",
    "file_read2 = file_read1.replace('The President.',' OBAMA:') #replacing all instances of proper string with upper\n",
    "file_read3 = file_read2.replace('Mr. Lehrer.',' LEHRER:') #replacing all instances of proper string with upper\n",
    "file_read4 = file_read3.replace('Mr. Schieffer.',' SCHIEFFER:') #replacing all instances of proper string with upper\n",
    "file_read5 = file_read4.replace('Ms. Crowley.',' CROWLEY:') #replacing all instances of proper string with upper\n",
    "file_read5\n",
    "MR_2012 = re.findall(r' ROMNEY:(.*?)(?:OBAMA:| LEHRER:| SCHIEFFER:| CROWLEY:)', file_read5) #extracting comments made by Romney only\n",
    "BO_2012 = re.findall(r' OBAMA:(.*?)(?:ROMNEY:| LEHRER:| SCHIEFFER:| CROWLEY:)', file_read5) #extracting comments made by Obama only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MR_2012_wc = ''.join(MR_2012)\n",
    "BO_2012_wc = ''.join(BO_2012)\n",
    "print(\"Barack Obama's word count in the 2012 presidential debates is \" + str(\"{:,}\".format(len(BO_2012_wc))) + \" words.\")\n",
    "print(\"Mitt Romney's word count in the 2012 presidential debates is \" + str(\"{:,}\".format(len(MR_2012_wc))) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 2016 Presidential Debate Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "for i in file_list:\n",
    "    if '2016' in i:\n",
    "        file = open(i,'rt')\n",
    "        file_read = file.read()\n",
    "        file_read = file_read.replace('\\n', '')\n",
    "        all_files.append(file_read)\n",
    "        print(\"\\n\")\n",
    "        print(125*\"+\")\n",
    "        print(file.name)\n",
    "        print(125*\"+\")\n",
    "        print(\"\\n\")\n",
    "        file.close()\n",
    "        print(file_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_files = ' '.join(all_files) #converting list to string\n",
    "cleaned_files = string_files.replace('\\n\\n','') #replacing new line tags\n",
    "file_read1 = cleaned_files.replace('Trump:',' TRUMP:') #replacing all instances of proper string with upper\n",
    "file_read2 = file_read1.replace('Clinton:',' CLINTON:') #replacing all instances of proper string with upper\n",
    "file_read3 = file_read2.replace('Holt:',' HOLT:') #replacing all instances of proper string with upper\n",
    "file_read4 = file_read3.replace('Wallace:',' WALLACE:') #replacing all instances of proper string with upper\n",
    "file_read5 = file_read4.replace('Raddatz:',' RADDATZ:') #replacing all instances of proper string with upper\n",
    "file_read6 = file_read5.replace('Cooper:',' COOPER:') #replacing all instances of proper string with upper\n",
    "DT_2016 = re.findall(r'TRUMP:(.*?)(?:CLINTON:|HOLT:|WALLACE:|RADDATZ:|COOPER:)', file_read6) #extracting comments made by Trump only\n",
    "HC_2016 = re.findall(r'CLINTON:(.*?)(?:TRUMP:|HOLT:|WALLACE:|RADDATZ:|COOPER:)', file_read6) #extracting comments made by Clinton only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_2016_wc = ''.join(DT_2016)\n",
    "HC_2016_wc = ''.join(HC_2016)\n",
    "print(\"Donald Trump's word count in the 2016 presidential debates is \" + str(\"{:,}\".format(len(DT_2016_wc))) + \" words.\")\n",
    "print(\"Hillary Clinton's word count in the 2016 presidential debates is \" + str(\"{:,}\".format(len(HC_2016_wc))) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Word Count (Without Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Without discarding any information, on a very superficial level, we can see that Obama has had the highest word count in a presidential debate followed by Romney in 2012.\n",
    "<br>\n",
    "\n",
    "| Year | Nominee | Word Count |\n",
    "| --- | --- | --- |\n",
    "| 2008 | <font color=blue>Obama<font color=black> | **121,759** |\n",
    "| 2008 | <font color=red>McCain<font color=black> | 112,023 |\n",
    "| 2012 | <font color=blue>Obama<font color=black> | **124,529** |\n",
    "| 2012 | <font color=red>Romney<font color=black> | 124,418 |\n",
    "| 2016 | <font color=blue>Clinton<font color=black> | 106,863 |\n",
    "| 2016 | <font color=red>Trump<font color=black> | **121,193** |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Word Count (With Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_data(data):\n",
    "    data = \"\".join(data)\n",
    "    data = word_tokenize(data)\n",
    "    data = [w for w in data if not w in stop_words]\n",
    "    wc_lst = len(data)\n",
    "    return wc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_files = [BO_2008,JM_2008,BO_2012,MR_2012,HC_2016,DT_2016]\n",
    "lst = ['Obama (2008)','McCain (2008)','Obama (2012)','Romney (2012)','Clinton (2016)','Trump (2016)']\n",
    "\n",
    "wc = []\n",
    "for i in wc_files:\n",
    "    wc_vals = cleaned_data(i)\n",
    "    wc.append(wc_vals)\n",
    "\n",
    "word_count = pd.DataFrame({'Nominee':lst,'Words':wc})\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = word_count.plot(kind='bar',x='Nominee',y='Words',figsize=(15,5),legend = True, fontsize = 12,color=['blue', 'red', 'blue', 'red', 'blue', 'red'])\n",
    "ax.set_xlabel(\"Nominee\", fontsize=12)\n",
    "ax.set_ylabel(\"Words\", fontsize=12)\n",
    "ax.get_legend().remove()\n",
    "ax.set_title('Count of words used by Nominees in Debates (after removal of stopwords)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Debate Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = str.maketrans('', '', string.punctuation)\n",
    "regex = r\"(?<!\\d)[....'--'``](?!\\d)\"\n",
    "\n",
    "def similarity(data):\n",
    "    data = \"\".join(data)\n",
    "    data = data.replace(\"...\",\" \")\n",
    "    data = data.replace(\".\",\" \")\n",
    "    data = re.sub(regex, \"\", data)\n",
    "    data = data.translate(punc)\n",
    "    data = word_tokenize(data)\n",
    "    data = [w for w in data if not w in stop_words]    \n",
    "    \n",
    "    return data\n",
    "\n",
    "def jacardian_distance(file1, file2):\n",
    "    file1 = similarity(file1)\n",
    "    file2 = similarity(file2)\n",
    "    intersection = len(list(set(file1).intersection(set(file2))))\n",
    "    union = (len(set(file1)) + len(set(file2))) - intersection\n",
    "    jacardian = float(intersection/ union)\n",
    "    \n",
    "    return jacardian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [BO_2008,JM_2008,BO_2012,MR_2012,HC_2016,DT_2016]\n",
    "\n",
    "jac_mat = np.zeros(shape = (len(file_list), len(file_list)))\n",
    "jac_mat = pd.DataFrame(jac_mat)#, index=6, columns=6)\n",
    "\n",
    "for a in range(len(file_list)):\n",
    "    for b in range(len(file_list)):\n",
    "        #print(jacardian_distance(file_list[a], file_list[b]))\n",
    "        jac_mat[a][b] = jacardian_distance(file_list[a], file_list[b])\n",
    "\n",
    "jac_mat = jac_mat.rename(columns={0 : \"BO_2008\",\n",
    "                                  1 : \"JM_2008\",\n",
    "                                  2 : \"BO_2012\",\n",
    "                                  3 : \"MR_2012\",\n",
    "                                  4 : \"HC_2016\",\n",
    "                                  5 : \"DT_2016\"},\n",
    "                        index = {0 : \"BO_2008\",\n",
    "                                  1 : \"JM_2008\",\n",
    "                                  2 : \"BO_2012\",\n",
    "                                  3 : \"MR_2012\",\n",
    "                                  4 : \"HC_2016\",\n",
    "                                  5 : \"DT_2016\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "# mask = np.zeros_like(corr)\n",
    "# mask[np.triu_indices_from(mask)] = True\n",
    "hm = sns.heatmap(jac_mat,\n",
    "                cbar=True,\n",
    "                square=True,\n",
    "                fmt='d',\n",
    "                annot_kws={'size': 15},\n",
    "                cmap=sns.cubehelix_palette(rot=-.57),\n",
    "                vmin=0, vmax=.6)\n",
    "\n",
    "# Show heat map\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
