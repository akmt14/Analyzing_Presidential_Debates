{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Presidential Debates\n",
    "\n",
    "## Group Members - Purvi Thakor, Junior Ovince, Akshay Kamath\n",
    "<br>\n",
    "With the midterms around the corner & politics dominating the news, we decided to analyze Presidential debates for our NLP project. We found datasets available on\n",
    "[UC Barbara's The American Presidency Project](https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/presidential-candidates-debates-1960-2016).\n",
    "\n",
    "We discussed & decided to analyze the presidential debates from 2002 onwards which meant we worked on 3 sets of debates - \n",
    "\n",
    "* <font color=blue>Gore<font color=black> v <font color=red>Bush<font color=black> _(2000)_\n",
    "  -  [Presidential Debate in St Louis, Missouri](https://www.presidency.ucsb.edu/ws/index.php?pid=29420)\n",
    "  -  [Presidential Debate in Winston-Salem, North Carolina](https://www.presidency.ucsb.edu/ws/index.php?pid=29419)\n",
    "  -  [Presidential Debate in Boston, Massachusetts](https://www.presidency.ucsb.edu/ws/index.php?pid=29418)\n",
    "<br>\n",
    "<br>\n",
    "* <font color=blue>Kerry<font color=black> v <font color=red>Bush<font color=black> _(2004)_\n",
    "  -  [Presidential Debate in Tempe, Arizona](https://www.presidency.ucsb.edu/ws/index.php?pid=63163)\n",
    "  -  [Presidential Debate in St Louis, Missouri](https://www.presidency.ucsb.edu/ws/index.php?pid=72776)\n",
    "  -  [Presidential Debate in Coral Gables, Florida](https://www.presidency.ucsb.edu/ws/index.php?pid=72770)\n",
    "<br>\n",
    "<br>\n",
    "* <font color=blue>Obama<font color=black> v <font color=red>McCain<font color=black> _(2008)_\n",
    "  -  [Presidential Debate in Hempstead, New York](https://www.presidency.ucsb.edu/ws/index.php?pid=84526)\n",
    "  -  [Presidential Debate in Nashville, Tennessee](https://www.presidency.ucsb.edu/ws/index.php?pid=84482)\n",
    "  -  [Presidential Debate in Oxford, Mississippi](https://www.presidency.ucsb.edu/ws/index.php?pid=78691)\n",
    "<br>\n",
    "<br>\n",
    "* <font color=blue>Obama<font color=black> v <font color=red>Romney<font color=black> _(2012)_\n",
    "  -  [Presidential Debate at Lynn University in Boca Raton, Florida](https://www.presidency.ucsb.edu/ws/index.php?pid=102344)\n",
    "  -  [Presidential Debate at Hofstra University in Hempstead, New York](https://www.presidency.ucsb.edu/ws/index.php?pid=102343)\n",
    "  -  [Presidential Debate at the University of Denver, Colorado](https://www.presidency.ucsb.edu/ws/index.php?pid=102317)\n",
    "<br>\n",
    "<br>\n",
    "* <font color=blue>Clinton<font color=black> v <font color=red>Trump<font color=black> _(2016)_\n",
    "  -  [Presidential Debate at the University of Nevada, Las Vegas](https://www.presidency.ucsb.edu/ws/index.php?pid=119039)\n",
    "  -  [Presidential Debate at Washington University in St. Louis, Missouri](https://www.presidency.ucsb.edu/ws/index.php?pid=119038)\n",
    "  -  [Presidential Debate at Hofstra University in Hempstead, New York](https://www.presidency.ucsb.edu/ws/index.php?pid=119012)\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "Furthermore, we decided to analyze both the victory & concession speeches for all the candidates.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The data was manually scraped from the abovementioned site & stored in text files. Thankfully, the format across each of the documents was more or less the same & we ended up saving a lot of time in the initial few steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import tabulate\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "from textstat.textstat import textstatistics, easy_word_set, legacy_round\n",
    "from nltk.stem import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkdir = os.chdir(\"D:/Project/NLP/presidential_debates/\")\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename).read()\n",
    "    return input_file_text\n",
    "\n",
    "file_list=[]\n",
    "for file in os.listdir(wrkdir):\n",
    "    file_list.append(file) \n",
    "\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in file_list:\n",
    "    file = open(i,'rt')\n",
    "    file_read = file.read()\n",
    "    print(\"\\n\")\n",
    "    print(125*\"+\")\n",
    "    print(file.name)\n",
    "    print(125*\"+\")\n",
    "    print(\"\\n\")\n",
    "    file.close()\n",
    "    print(file_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the format of the files is slightly different. Initially, the conversation begins with the speaker's name in proper format & then it gets converted to upper format. So we realized that each document needs to be cleansed separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2000 Presidential Debate Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "for i in file_list:\n",
    "    if '2000' in i:\n",
    "        file = open(i,'rt')\n",
    "        file_read = file.read()\n",
    "        file_read = file_read.replace('\\n', '')\n",
    "#        file_read = file_read.replace(\".\",\". \")\n",
    "        all_files.append(file_read)\n",
    "        print(\"\\n\")\n",
    "        print(125*\"+\")\n",
    "        print(file.name)\n",
    "        print(125*\"+\")\n",
    "        print(\"\\n\")\n",
    "        file.close()\n",
    "        print(file_read)\n",
    "        \n",
    "string_files = ' '.join(all_files) #converting list to string\n",
    "cleaned_files = string_files.replace('\\n\\n','') #replacing new line tags\n",
    "file_read1 = cleaned_files.replace('Senator Kerry.','KERRY:') #replacing one instance of proper string with upper\n",
    "file_read2 = file_read1.replace('President Bush.','BUSH:') #replacing one instance of proper string with upper\n",
    "GB_2000 = re.findall(r'BUSH:(.*?)(?:GORE:|LEHRER:|SCHIEFFER:|MODERATOR:|MEMBER OF AUDIENCE:)', file_read2) #extracting comments made by McCain only\n",
    "AG_2000 = re.findall(r'GORE:(.*?)(?:BUSH:|LEHRER:|SCHIEFFER:|MODERATOR:|MEMBER OF AUDIENCE:)', file_read2) #extracting comments made by Obama only\n",
    "\n",
    "GB_2000_wc = ''.join(GB_2000)\n",
    "AG_2000_wc = ''.join(AG_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"George Bush's word count in the 2000 presidential debates is \" + str(\"{:,}\".format(len(GB_2000_wc))) + \" words.\")\n",
    "print(\"Al Gore's word count in the 2000 presidential debates is \" + str(\"{:,}\".format(len(AG_2000_wc))) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 2004 Presidential Debate Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "for i in file_list:\n",
    "    if '2004' in i:\n",
    "        file = open(i,'rt')\n",
    "        file_read = file.read()\n",
    "        file_read = file_read.replace('\\n', '')\n",
    "        all_files.append(file_read)\n",
    "        print(\"\\n\")\n",
    "        print(125*\"+\")\n",
    "        print(file.name)\n",
    "        print(125*\"+\")\n",
    "        print(\"\\n\")\n",
    "        file.close()\n",
    "        print(file_read)\n",
    "        \n",
    "string_files = ' '.join(all_files) #converting list to string\n",
    "cleaned_files = string_files.replace('\\n\\n','') #replacing new line tags\n",
    "file_read1 = cleaned_files.replace('Kerry:','KERRY: ') #replacing one instance of proper string with upper\n",
    "file_read2 = file_read1.replace('Bush: ','BUSH: ') #replacing one instance of proper string with upper\n",
    "GB_2004 = re.findall(r'BUSH:(.*?)(?:KERRY:|LEHRER:|SCHIEFFER:|MODERATOR:|MEMBER OF AUDIENCE:)', file_read2) #extracting comments made by McCain only\n",
    "JK_2004 = re.findall(r'KERRY:(.*?)(?:BUSH:|LEHRER:|SCHIEFFER:|MODERATOR:|MEMBER OF AUDIENCE:)', file_read2) #extracting comments made by Obama only\n",
    "\n",
    "GB_2004_wc = ''.join(GB_2004)\n",
    "JK_2004_wc = ''.join(JK_2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"George Bush's word count in the 2004 presidential debates is \" + str(\"{:,}\".format(len(GB_2004_wc))) + \" words.\")\n",
    "print(\"John Kerry's word count in the 2004 presidential debates is \" + str(\"{:,}\".format(len(JK_2004_wc))) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 2008 Presidential Debate Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "for i in file_list:\n",
    "    if '2008' in i:\n",
    "        file = open(i,'rt')\n",
    "        file_read = file.read()\n",
    "        file_read = file_read.replace('\\n', '')\n",
    "        all_files.append(file_read)\n",
    "        print(\"\\n\")\n",
    "        print(125*\"+\")\n",
    "        print(file.name)\n",
    "        print(125*\"+\")\n",
    "        print(\"\\n\")\n",
    "        file.close()\n",
    "        print(file_read)\n",
    "        \n",
    "string_files = ' '.join(all_files) #converting list to string\n",
    "cleaned_files = string_files.replace('\\n\\n','') #replacing new line tags\n",
    "file_read1 = cleaned_files.replace('McCain:','MCCAIN:') #replacing one instance of proper string with upper\n",
    "file_read2 = file_read1.replace('Obama:','OBAMA:') #replacing one instance of proper string with upper\n",
    "JM_2008 = re.findall(r'MCCAIN:(.*?)(?:OBAMA:|LEHRER:|SCHIEFFER:|BROKAW:)', file_read2) #extracting comments made by McCain only\n",
    "BO_2008 = re.findall(r'OBAMA:(.*?)(?:MCCAIN:|LEHRER:|SCHIEFFER:|BROKAW:)', file_read2) #extracting comments made by Obama only\n",
    "\n",
    "JM_2008_wc = ''.join(JM_2008)\n",
    "BO_2008_wc = ''.join(BO_2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Barack Obama's word count in the 2008 presidential debates is \" + str(\"{:,}\".format(len(BO_2008_wc))) + \" words.\")\n",
    "print(\"John McCain's word count in the 2008 presidential debates is \" + str(\"{:,}\".format(len(JM_2008_wc))) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 2012 Presidential Debate Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "for i in file_list:\n",
    "    if '2012' in i:\n",
    "        file = open(i,'rt')\n",
    "        file_read = file.read()\n",
    "        file_read = file_read.replace('\\n', '')\n",
    "        all_files.append(file_read)\n",
    "        print(\"\\n\")\n",
    "        print(125*\"+\")\n",
    "        print(file.name)\n",
    "        print(125*\"+\")\n",
    "        print(\"\\n\")\n",
    "        file.close()\n",
    "        print(file_read)\n",
    "\n",
    "string_files = ' '.join(all_files) #converting list to string\n",
    "cleaned_files = string_files.replace('\\n\\n','') #replacing new line tags\n",
    "file_read1 = cleaned_files.replace('Gov. Romney.',' ROMNEY:') #replacing all instances of proper string with upper\n",
    "file_read2 = file_read1.replace('The President.',' OBAMA:') #replacing all instances of proper string with upper\n",
    "file_read3 = file_read2.replace('Mr. Lehrer.',' LEHRER:') #replacing all instances of proper string with upper\n",
    "file_read4 = file_read3.replace('Mr. Schieffer.',' SCHIEFFER:') #replacing all instances of proper string with upper\n",
    "file_read5 = file_read4.replace('Ms. Crowley.',' CROWLEY:') #replacing all instances of proper string with upper\n",
    "\n",
    "MR_2012 = re.findall(r' ROMNEY:(.*?)(?:OBAMA:| LEHRER:| SCHIEFFER:| CROWLEY:)', file_read5) #extracting comments made by Romney only\n",
    "BO_2012 = re.findall(r' OBAMA:(.*?)(?:ROMNEY:| LEHRER:| SCHIEFFER:| CROWLEY:)', file_read5) #extracting comments made by Obama only\n",
    "\n",
    "MR_2012_wc = ''.join(MR_2012)\n",
    "BO_2012_wc = ''.join(BO_2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Barack Obama's word count in the 2012 presidential debates is \" + str(\"{:,}\".format(len(BO_2012_wc))) + \" words.\")\n",
    "print(\"Mitt Romney's word count in the 2012 presidential debates is \" + str(\"{:,}\".format(len(MR_2012_wc))) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 2016 Presidential Debate Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "for i in file_list:\n",
    "    if '2016' in i:\n",
    "        file = open(i,'rt')\n",
    "        file_read = file.read()\n",
    "        file_read = file_read.replace('\\n', '')\n",
    "        all_files.append(file_read)\n",
    "        print(\"\\n\")\n",
    "        print(125*\"+\")\n",
    "        print(file.name)\n",
    "        print(125*\"+\")\n",
    "        print(\"\\n\")\n",
    "        file.close()\n",
    "        print(file_read)\n",
    "\n",
    "string_files = ' '.join(all_files) #converting list to string\n",
    "cleaned_files = string_files.replace('\\n\\n','') #replacing new line tags\n",
    "file_read1 = cleaned_files.replace('Trump:',' TRUMP:') #replacing all instances of proper string with upper\n",
    "file_read2 = file_read1.replace('Clinton:',' CLINTON:') #replacing all instances of proper string with upper\n",
    "file_read3 = file_read2.replace('Holt:',' HOLT:') #replacing all instances of proper string with upper\n",
    "file_read4 = file_read3.replace('Wallace:',' WALLACE:') #replacing all instances of proper string with upper\n",
    "file_read5 = file_read4.replace('Raddatz:',' RADDATZ:') #replacing all instances of proper string with upper\n",
    "file_read6 = file_read5.replace('Cooper:',' COOPER:') #replacing all instances of proper string with upper\n",
    "\n",
    "DT_2016 = re.findall(r'TRUMP:(.*?)(?:CLINTON:|HOLT:|WALLACE:|RADDATZ:|COOPER:)', file_read6) #extracting comments made by Trump only\n",
    "HC_2016 = re.findall(r'CLINTON:(.*?)(?:TRUMP:|HOLT:|WALLACE:|RADDATZ:|COOPER:)', file_read6) #extracting comments made by Clinton only\n",
    "\n",
    "DT_2016_wc = ''.join(DT_2016)\n",
    "HC_2016_wc = ''.join(HC_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Donald Trump's word count in the 2016 presidential debates is \" + str(\"{:,}\".format(len(DT_2016_wc))) + \" words.\")\n",
    "print(\"Hillary Clinton's word count in the 2016 presidential debates is \" + str(\"{:,}\".format(len(HC_2016_wc))) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Word Count (Without Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Without discarding any information, on a very superficial level, we can see that Obama has had the highest word count in a presidential debate followed by Romney in 2012.\n",
    "<br>\n",
    "\n",
    "George Bush's word count in the 2004 presidential debates is 105,301 words.\n",
    "John Kerry's word count in the 2004 presidential debates is 79,328 words.\n",
    "\n",
    "| Year | Nominee | Word Count |\n",
    "| --- | --- | --- |\n",
    "| 2000 | <font color=blue>Gore<font color=black> | 108,651 |\n",
    "| **2000** | <font color=red>**Bush**<font color=black> | **116,400** |\n",
    "| 2004 | <font color=blue>Kerry<font color=black> | 79,328 |\n",
    "| **2004** | <font color=red>**Bush**<font color=black> | **105,301** |    \n",
    "| **2008** | <font color=blue>**Obama**<font color=black> | **121,759** |\n",
    "| 2008 | <font color=red>McCain<font color=black> | 112,023 |    \n",
    "| **2012** | <font color=blue>**Obama**<font color=black> | **124,529** |\n",
    "| 2012 | <font color=red>Romney<font color=black> | 124,418 |    \n",
    "| 2016 | <font color=blue>Clinton<font color=black> | 106,863 |\n",
    "| **2016** | <font color=red>**Trump**<font color=black> | **121,193** |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Word Count (With Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_data(data):\n",
    "    data = \"\".join(data)\n",
    "    data = word_tokenize(data)\n",
    "    data = [w for w in data if not w in stop_words]\n",
    "    wc_lst = len(data)\n",
    "    return wc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_files = [AG_2000,GB_2000,JK_2004,GB_2004,BO_2008,JM_2008,BO_2012,MR_2012,HC_2016,DT_2016]\n",
    "lst = ['Gore (2000)','Bush (2000)','Kerry (2004)','Bush (2004)','Obama (2008)','McCain (2008)','Obama (2012)','Romney (2012)','Clinton (2016)','Trump (2016)']\n",
    "\n",
    "wc = []\n",
    "for i in wc_files:\n",
    "    wc_vals = cleaned_data(i)\n",
    "    wc.append(wc_vals)\n",
    "\n",
    "word_count = pd.DataFrame({'Nominee':lst,'Words':wc})\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = word_count.plot(kind='bar',x='Nominee',y='Words',figsize=(15,5),legend = True, fontsize = 12,color=['blue', 'red', 'blue', 'red', 'blue', 'red','blue', 'red', 'blue','red'])\n",
    "ax.set_xlabel(\"Nominee\", fontsize=12)\n",
    "ax.set_ylabel(\"Words\", fontsize=12)\n",
    "ax.get_legend().remove()\n",
    "ax.set_title('Count of words used by Nominees in Debates (after removal of stopwords)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Debate Similarity\n",
    "\n",
    "#### 1) Jacardian Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = str.maketrans('', '', string.punctuation)\n",
    "regex = r\"(?<!\\d)[....'--'``](?!\\d)\"\n",
    "\n",
    "def cleaner(data):\n",
    "    data = \"\".join(data)\n",
    "    data = data.replace(\"...\",\" \")\n",
    "    data = data.replace(\".\",\" \")\n",
    "    data = re.sub(regex, \"\", data)\n",
    "    data = data.translate(punc)\n",
    "    data = word_tokenize(data)\n",
    "    data = [w for w in data if not w in stop_words]    \n",
    "    \n",
    "    return data\n",
    "\n",
    "def jacardian_distance(file1, file2):\n",
    "    file1 = cleaner(file1)\n",
    "    file2 = cleaner(file2)\n",
    "    intersection = len(list(set(file1).intersection(set(file2))))\n",
    "    union = (len(set(file1)) + len(set(file2))) - intersection\n",
    "    jacardian = float(intersection/ union)\n",
    "    \n",
    "    return jacardian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [AG_2000,GB_2000,JK_2004,GB_2004,BO_2008,JM_2008,BO_2012,MR_2012,HC_2016,DT_2016]\n",
    "\n",
    "jac_mat = np.zeros(shape = (len(file_list), len(file_list)))\n",
    "jac_mat = pd.DataFrame(jac_mat)#, index=6, columns=6)\n",
    "\n",
    "for a in range(len(file_list)):\n",
    "    for b in range(len(file_list)):\n",
    "        #print(jacardian_distance(file_list[a], file_list[b]))\n",
    "        jac_mat[a][b] = jacardian_distance(file_list[a], file_list[b])\n",
    "\n",
    "jac_mat = jac_mat.rename(columns={0 : \"Gore'00\",\n",
    "                                  1 : \"Bush'00\",\n",
    "                                  2 : \"Kerry'04\",\n",
    "                                  3 : \"Bush'04\",\n",
    "                                  4 : \"Obama'08\",\n",
    "                                  5 : \"McCain'08\",\n",
    "                                  6 : \"Obama'12\",\n",
    "                                  7 : \"Romney'12\",\n",
    "                                  8 : \"Clinton'16\",\n",
    "                                  9 : \"Trump'16\"\n",
    "                                 },\n",
    "                        index =  {0 : \"Gore'00\",\n",
    "                                  1 : \"Bush'00\",\n",
    "                                  2 : \"Kerry'04\",\n",
    "                                  3 : \"Bush'04\",\n",
    "                                  4 : \"Obama'08\",\n",
    "                                  5 : \"McCain'08\",\n",
    "                                  6 : \"Obama'12\",\n",
    "                                  7 : \"Romney'12\",\n",
    "                                  8 : \"Clinton'16\",\n",
    "                                  9 : \"Trump'16\"\n",
    "                                 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "mask =  np.tri(jac_mat.shape[1],k=-1)\n",
    "hm_j = sns.heatmap(jac_mat,\n",
    "                cbar=True,\n",
    "                square=True,\n",
    "#                fmt='d',\n",
    "                annot = True,\n",
    "                annot_kws={'size': 15},\n",
    "                cmap=sns.cubehelix_palette(rot=-.57),\n",
    "                mask = mask.T,\n",
    "                vmin = 0.2, vmax = .5\n",
    "                )\n",
    "\n",
    "# Show heat map\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(file1, file2):\n",
    "    \n",
    "    file1 = cleaner(file1)\n",
    "    file2 = cleaner(file2)\n",
    "\n",
    "    combined = sorted(list(set(file1 + file2)), key=str.lower)\n",
    "    \n",
    "    counts_1 = {}\n",
    "    counts_2 = {}\n",
    "    \n",
    "    for i in combined:\n",
    "        counts_1[i]=file1.count(i)\n",
    "        counts_2[i]=file2.count(i)\n",
    "    \n",
    "    document_1_vector = np.array( list(counts_1.values() ) ) # len is same as combined\n",
    "    document_2_vector = np.array( list(counts_2.values() ) )  # len is same as combined\n",
    "    \n",
    "    dot_product_of_two_document_vectors = np.dot(document_1_vector, document_2_vector)\n",
    "    \n",
    "    norm_1 = np.linalg.norm(document_1_vector)\n",
    "    norm_2 = np.linalg.norm(document_2_vector)\n",
    "    \n",
    "    cosine_similarity = dot_product_of_two_document_vectors / (norm_1 * norm_2)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_list = [BO_2008,JM_2008,BO_2012,MR_2012,HC_2016,DT_2016]\n",
    "\n",
    "cos_mat = np.zeros(shape = (len(file_list), len(file_list)))\n",
    "cos_mat = pd.DataFrame(cos_mat)#, index=6, columns=6)\n",
    "\n",
    "for a in range(len(file_list)):\n",
    "    for b in range(len(file_list)):\n",
    "        cos_mat[a][b] = cosine_similarity(file_list[a], file_list[b])\n",
    "\n",
    "cos_mat = cos_mat.rename(columns={0 : \"Gore'00\",\n",
    "                                  1 : \"Bush'00\",\n",
    "                                  2 : \"Kerry'04\",\n",
    "                                  3 : \"Bush'04\",\n",
    "                                  4 : \"Obama'08\",\n",
    "                                  5 : \"McCain'08\",\n",
    "                                  6 : \"Obama'12\",\n",
    "                                  7 : \"Romney'12\",\n",
    "                                  8 : \"Clinton'16\",\n",
    "                                  9 : \"Trump'16\"\n",
    "                                 },\n",
    "                        index =  {0 : \"Gore'00\",\n",
    "                                  1 : \"Bush'00\",\n",
    "                                  2 : \"Kerry'04\",\n",
    "                                  3 : \"Bush'04\",\n",
    "                                  4 : \"Obama'08\",\n",
    "                                  5 : \"McCain'08\",\n",
    "                                  6 : \"Obama'12\",\n",
    "                                  7 : \"Romney'12\",\n",
    "                                  8 : \"Clinton'16\",\n",
    "                                  9 : \"Trump'16\"\n",
    "                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "# mask = np.zeros_like(corr)\n",
    "# mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "mask =  np.tri(cos_mat.shape[1],k=-1)\n",
    "\n",
    "hm_c = sns.heatmap(cos_mat,\n",
    "                cbar=True,\n",
    "                square=True,\n",
    "                annot = True,\n",
    "                cmap=sns.cubehelix_palette(rot=-.7),\n",
    "                annot_kws={'size': 15},\n",
    "                mask=mask.T,\n",
    "                vmin=.7,vmax=1)\n",
    "\n",
    "# Show heat map\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADJACENT SIMILARITY PLOTS\n",
    "\n",
    "# fig, ax = plt.subplots(1,2)#,figsize=(10,10)\n",
    "# fig.set_size_inches(10,10)\n",
    "# hm_j = sns.heatmap(jac_mat,\n",
    "#                 cbar=True,\n",
    "#                 square=True,\n",
    "#                 annot = True,\n",
    "#                 cmap=sns.cubehelix_palette(rot=-.57),\n",
    "#                 annot_kws={'size': 15},\n",
    "#                 mask=mask.T,\n",
    "#                 ax=ax[0])\n",
    "\n",
    "# hm_c = sns.heatmap(cos_mat,\n",
    "#                 cbar=True,\n",
    "#                 square=True,\n",
    "#                 annot = True,\n",
    "#                 cmap=sns.cubehelix_palette(rot=-.57),\n",
    "#                 annot_kws={'size': 15},\n",
    "#                 mask=mask.T,\n",
    "#                 ax=ax[1])\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability Index\n",
    "\n",
    "#### The Flesch Reading Ease Readability Formula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def break_sentences(text):\n",
    "    document = nlp(text)\n",
    "    return document\n",
    "    \n",
    "def word_count(text): \n",
    "    sentences = break_sentences(text) \n",
    "    words = 0\n",
    "    for sentence in sentences: \n",
    "        words += len([token for token in sentence]) \n",
    "    return words\n",
    "\n",
    "def sentence_count(text): \n",
    "    sentences = break_sentences(text) \n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(word_count(BO_2008_wc)) + str(\" word count\"))\n",
    "print(str(sentence_count(BO_2008_wc)) + str(\" sentence count\"))\n",
    "\n",
    "#nlp(BO_2008_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = break_sentences(BO_2008_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
